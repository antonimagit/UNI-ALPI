{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************** INITIAL PARAMETERS ***************\n",
    "#*** MILVUS CONFIG ***\n",
    "MILVUS_URL = \"http://localhost\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "MILVUS_HOST = \"localhost\"\n",
    "MILVUS_ALIAS = \"default\"\n",
    "COLLECTION_NAME = \"samplecollection\"\n",
    "COLLECTION_DESCR = \"Description for samplecollection\"\n",
    "DELETE_COLLECTION_IF_EXISTS = True\n",
    "\n",
    "#*** WATSONX CONFIG ***\n",
    "WX_URL = \"https://us-south.ml.cloud.ibm.com\"\n",
    "WX_API_KEY = \"fpMu8viNLafvrayEkNyXxKjyYWOz9rFqzFBbg47-6U2i\"\n",
    "WX_PROJECT_ID = '05a78408-7b0c-4c6f-ad34-5e267488200c'\n",
    "WX_EMBEDING_MODEL = \"intfloat/multilingual-e5-large\"\n",
    "WX_EMBEDING_MODEL_DIM = 1024\n",
    "\n",
    "#*** DOCUMENT TO IMPORT CONFIG ***\n",
    "DOCUMENT_FOLDER_PATH = \"/Users/antoniomarinelli/Desktop/Python Mongodb/scraped_texts/UNIALPI/2025_10_01_18_00_00\"\n",
    "DOCUMENT_CSV_FILE = \"scraped_pages.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************** CREATE CONNECTION ***************\n",
    "from pymilvus import MilvusClient, connections, db\n",
    "\n",
    "milvusClient = MilvusClient(\n",
    "    uri=MILVUS_URL + \":\" + MILVUS_PORT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************** CREATE COLLECTION + INDEX ***************\n",
    "\n",
    "from pymilvus import MilvusClient, DataType, FieldSchema, CollectionSchema\n",
    "milvusClient = MilvusClient(uri=MILVUS_URL + \":\" + MILVUS_PORT)\n",
    "\n",
    "id_field = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, description=\"primary id\")\n",
    "docPath = FieldSchema(name=\"docPath\", dtype=DataType.VARCHAR, max_length=1024)\n",
    "documentName = FieldSchema(name=\"documentName\", dtype=DataType.VARCHAR, max_length=1024)\n",
    "section = FieldSchema(name=\"section\", dtype=DataType.VARCHAR, max_length=1024)\n",
    "link = FieldSchema(name=\"link\", dtype=DataType.VARCHAR, max_length=1024)\n",
    "language = FieldSchema(name=\"language\", dtype=DataType.VARCHAR, max_length=100)\n",
    "depth = FieldSchema(name=\"depth\", dtype=DataType.INT16)\n",
    "documentType = FieldSchema(name=\"documentType\", dtype=DataType.VARCHAR, max_length=100)\n",
    "contentChunk = FieldSchema(name=\"contentChunk\", dtype=DataType.VARCHAR, max_length=40960)\n",
    "contentPage = FieldSchema(name=\"contentPage\", dtype=DataType.VARCHAR, max_length=65535)\n",
    "numChunks = FieldSchema(name=\"numChunks\", dtype=DataType.INT16)\n",
    "indexChunk = FieldSchema(name=\"indexChunk\", dtype=DataType.INT16)\n",
    "vectorDoc = FieldSchema(name=\"vectorDoc\", dtype=DataType.FLOAT_VECTOR, dim=1024)\n",
    "\n",
    "mySchema = CollectionSchema(\n",
    "    fields=[id_field, docPath, documentName, section, link, language, depth,\n",
    "            documentType, contentChunk, contentPage, numChunks, indexChunk, vectorDoc],\n",
    "    auto_id=True,\n",
    "    enable_dynamic_field=True,\n",
    "    description=COLLECTION_DESCR\n",
    ")\n",
    "\n",
    "if milvusClient.has_collection(COLLECTION_NAME):\n",
    "    print(\"⚠️ Collection \" + COLLECTION_NAME + \" already exists.\")\n",
    "    if(DELETE_COLLECTION_IF_EXISTS == True):\n",
    "        milvusClient.drop_collection(COLLECTION_NAME)\n",
    "        print(\"⚠️ Collection \" + COLLECTION_NAME + \" delete succesfully.\")\n",
    "\n",
    "milvusClient.create_collection(\n",
    "    collection_name=COLLECTION_NAME, \n",
    "    schema=mySchema\n",
    ")\n",
    "print(\"✅ Collection \" + COLLECTION_NAME + \" created succesfully.\")\n",
    "\n",
    "index_params = milvusClient.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"vectorDoc\",\n",
    "    index_type=\"HNSW\",\n",
    "    metric_type=\"COSINE\",\n",
    "    params={\n",
    "        \"M\": 16,\n",
    "        \"efConstruction\": 200\n",
    "    }\n",
    ")\n",
    "#CREATE INDEX\n",
    "milvusClient.create_index(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    index_params=index_params\n",
    ")\n",
    "print(\"✅ Index created succesfully.\")\n",
    "\n",
    "milvusClient.load_collection(collection_name=COLLECTION_NAME)\n",
    "print(\"✅ Collection loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "from pymilvus import Collection, MilvusClient, connections\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams\n",
    "from ibm_watsonx_ai.foundation_models import Embeddings\n",
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from datetime import datetime\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "import pdfplumber\n",
    "\n",
    "collectionName = COLLECTION_NAME\n",
    "minChunkSize = 200\n",
    "\n",
    "useWx = True\n",
    "useLocal = True\n",
    "\n",
    "modelIdParam = WX_EMBEDING_MODEL\n",
    "\n",
    "modelDimension = WX_EMBEDING_MODEL_DIM\n",
    "model = None\n",
    "\n",
    "credentials = Credentials(url=WX_URL, api_key=WX_API_KEY)\n",
    "ai_client = APIClient(credentials)\n",
    "embed_params = {\n",
    "    EmbedParams.TRUNCATE_INPUT_TOKENS: 512,\n",
    "    EmbedParams.RETURN_OPTIONS: {'input_text': True}\n",
    "}\n",
    "embedding = Embeddings(\n",
    "    model_id=modelIdParam,\n",
    "    credentials=ai_client.credentials,\n",
    "    params=embed_params,\n",
    "    project_id=WX_PROJECT_ID,\n",
    "    verify=False\n",
    ")\n",
    "\n",
    "connections.connect(alias=MILVUS_ALIAS, host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "milvusClient = MilvusClient(uri=MILVUS_URL)\n",
    "\n",
    "def clean_text(text):\n",
    "    return \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
    "\n",
    "def EmbeddingText_WX(texts):\n",
    "    try:\n",
    "        embedding_vectors = embedding.embed_documents(texts=texts)\n",
    "        return embedding_vectors[0]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Watsonx embedding error: {e}\")\n",
    "        return None\n",
    "\n",
    "def EmbeddingText_ST(texts):\n",
    "    return model.encode(texts)\n",
    "\n",
    "def GetChunks(text):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "def DeleteDocumentsInCollection(documentName):\n",
    "    collection = Collection(collectionName)\n",
    "    collection.delete(expr=f\"documentName == '{documentName}'\")\n",
    "    collection.flush()\n",
    "    time.sleep(2)\n",
    "\n",
    "def extract_language_from_url(url):\n",
    "    try:\n",
    "        path_parts = urlparse(url).path.strip(\"/\").split(\"/\")\n",
    "        if path_parts:\n",
    "            lang = path_parts[0].lower()\n",
    "            if lang in [\"it\", \"en\", \"fr\", \"de\", \"es\", \"pt\"]: \n",
    "                return lang\n",
    "        return \"unknown\"\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "def table_to_markdown(table):\n",
    "    if not table or not table[0]:\n",
    "        return \"\"\n",
    "    \n",
    "    table = [[cell if cell else \"\" for cell in row] for row in table]\n",
    "    col_widths = [max(len(str(row[i])) for row in table) for i in range(len(table[0]))]\n",
    "    \n",
    "    markdown_rows = []\n",
    "    header = \"| \" + \" | \".join(str(cell).ljust(col_widths[i]) for i, cell in enumerate(table[0])) + \" |\"\n",
    "    markdown_rows.append(header)\n",
    "    \n",
    "    separator = \"|\" + \"|\".join(\"-\" * (width + 2) for width in col_widths) + \"|\"\n",
    "    markdown_rows.append(separator)\n",
    "    \n",
    "    for row in table[1:]:\n",
    "        row_str = \"| \" + \" | \".join(str(cell).ljust(col_widths[i]) for i, cell in enumerate(row)) + \" |\"\n",
    "        markdown_rows.append(row_str)\n",
    "    \n",
    "    return \"\\n\".join(markdown_rows)\n",
    "\n",
    "def pdf_to_markdown(pdf_path):\n",
    "    markdown_content = []\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text() or \"\"\n",
    "            tables = page.extract_tables()\n",
    "            \n",
    "            if text:\n",
    "                markdown_content.append(text)\n",
    "            \n",
    "            if tables:\n",
    "                for table in tables:\n",
    "                    markdown_content.append(\"\\n\" + table_to_markdown(table) + \"\\n\")\n",
    "    \n",
    "    return \"\\n\".join(markdown_content)\n",
    "\n",
    "def ProcessTxtFile(documentPath, url, depth, language, documentType):\n",
    "    documentName = os.path.basename(documentPath)\n",
    "    with open(documentPath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = clean_text(f.read())\n",
    "\n",
    "    DeleteDocumentsInCollection(documentName)\n",
    "    chunks = [clean_text(c) for c in GetChunks(content)]\n",
    "    numChunks = len(chunks)\n",
    "\n",
    "    MAX_CONTENT_LENGTH = 64000\n",
    "    content_page = content[:MAX_CONTENT_LENGTH] if len(content) > MAX_CONTENT_LENGTH else content\n",
    "  \n",
    "    for indexChunk, chunk in enumerate(chunks, start=1):\n",
    "        if len(chunk) < minChunkSize:\n",
    "            continue\n",
    "        embedding_result = (\n",
    "            EmbeddingText_WX([chunk]) if useWx else EmbeddingText_ST([chunk])[0]\n",
    "        )\n",
    "        data = [{\n",
    "            \"docPath\": DOCUMENT_FOLDER_PATH,\n",
    "            \"documentName\": documentName,\n",
    "            \"section\": 'webpage',\n",
    "            \"link\": url,\n",
    "            \"language\": language,\n",
    "            \"depth\": int(depth),\n",
    "            \"documentType\": documentType,\n",
    "            \"contentChunk\": chunk,\n",
    "            \"contentPage\": content_page,\n",
    "            \"numChunks\": numChunks,\n",
    "            \"indexChunk\": indexChunk,\n",
    "            \"vectorDoc\": embedding_result\n",
    "        }]\n",
    "        milvusClient.insert(collection_name=collectionName, data=data)\n",
    "\n",
    "def ProcessPdfFile(documentPath, url, depth, language, documentType):\n",
    "    documentName = os.path.basename(documentPath)\n",
    "    markdown_text = pdf_to_markdown(documentPath)\n",
    "    \n",
    "    MAX_CONTENT_LENGTH = 64000\n",
    "    content_page = markdown_text[:MAX_CONTENT_LENGTH] if len(markdown_text) > MAX_CONTENT_LENGTH else markdown_text\n",
    "    \n",
    "    \n",
    "    DeleteDocumentsInCollection(documentName)\n",
    "    chunks = [clean_text(c) for c in GetChunks(markdown_text)]\n",
    "    numChunks = len(chunks)\n",
    "\n",
    "    for indexChunk, chunk in enumerate(chunks, start=1):\n",
    "        if len(chunk) < minChunkSize:\n",
    "            continue\n",
    "\n",
    "        embedding_result = (\n",
    "            EmbeddingText_WX([chunk]) if useWx else EmbeddingText_ST([chunk])[0]\n",
    "        )\n",
    "        \n",
    "        data = [{\n",
    "            \"docPath\": DOCUMENT_FOLDER_PATH,\n",
    "            \"documentName\": documentName,\n",
    "            \"section\": 'pdf',\n",
    "            \"link\": url,\n",
    "            \"language\": language,\n",
    "            \"depth\": int(depth),\n",
    "            \"documentType\": documentType,\n",
    "            \"contentChunk\": chunk,\n",
    "            \"contentPage\": content_page,\n",
    "            \"numChunks\": numChunks,\n",
    "            \"indexChunk\": indexChunk,\n",
    "            \"vectorDoc\": embedding_result\n",
    "        }]\n",
    "        milvusClient.insert(collection_name=collectionName, data=data)\n",
    "\n",
    "def StartImportProcess():\n",
    "    startProcess = datetime.now()\n",
    "    numProcessed = 0\n",
    "    numSkipped = 0\n",
    "    \n",
    "    print(\"Process started at \" + str(startProcess))\n",
    "\n",
    "    csvPath = os.path.join(DOCUMENT_FOLDER_PATH, DOCUMENT_CSV_FILE)\n",
    "    if not os.path.exists(csvPath):\n",
    "        print(f\"CSV file not found: {csvPath}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    with open(csvPath, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            try:\n",
    "                fileName = row[\"Text File\"]\n",
    "                url = row[\"URL\"]\n",
    "                depth = row.get(\"Depth\", \"0\")\n",
    "                language = extract_language_from_url(url)\n",
    "                documentType = row[\"Content-Type\"]\n",
    "\n",
    "                if not fileName:\n",
    "                    continue\n",
    "\n",
    "                filePath = os.path.join(DOCUMENT_FOLDER_PATH, fileName)\n",
    "                if not os.path.exists(filePath):\n",
    "                    logging.warning(f\"File not found: {filePath}\")\n",
    "                    numSkipped += 1\n",
    "                    continue\n",
    "\n",
    "                print(f\"Processing {fileName} (URL: {url})...\")\n",
    "                ext = os.path.splitext(fileName)[1].lower()\n",
    "                if ext == \".pdf\":\n",
    "                    ProcessPdfFile(filePath, url, depth, language, documentType)\n",
    "                elif ext == \".txt\":\n",
    "                    ProcessTxtFile(filePath, url, depth, language, documentType)\n",
    "                else:\n",
    "                    logging.warning(f\"Unsupported file type: {fileName}\")\n",
    "                    numSkipped += 1\n",
    "                    continue\n",
    "\n",
    "                numProcessed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing row: {e}\")\n",
    "                numSkipped += 1\n",
    "\n",
    "    stopProcess = datetime.now()\n",
    "    print(\"*\" * 40)\n",
    "    print(f\"Process started at: {startProcess}\")\n",
    "    print(f\"Process ended at: {stopProcess}\")\n",
    "    print(f\"Processed: {numProcessed} files\")\n",
    "    print(f\"Skipped: {numSkipped} files\")\n",
    "    print(f\"Duration: {(stopProcess - startProcess).seconds / 60:.2f} minutes\")\n",
    "    \n",
    "\n",
    "StartImportProcess()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
